{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "algo_builder_functions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB6DR9szjR7A"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMw8oAsKe5FC"
      },
      "source": [
        "**Select meteorological wrf model and meteorological station**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaGd-WTpdE61"
      },
      "source": [
        "def select_metmodel_station():\n",
        "    \"\"\" Select meteorological model and meteorological station \"\"\"\n",
        "    \n",
        "    \n",
        "    import numpy as np\n",
        "    import os\n",
        "    \n",
        "    \n",
        "    #select meteorogical model\n",
        "    wrf_directory=\"/content/drive/MyDrive/Colab Notebooks/ML_database/wrf_database/\"\n",
        "    met_models=[filename for filename in os.listdir(wrf_directory) \n",
        "               if (filename.endswith('.csv') or filename.endswith('.pat')) and filename.startswith(\"l\")]\n",
        "    print(\"Select meteorological model number\\n\")\n",
        "    for n,mod in zip(np.arange(0,len(met_models)),met_models):\n",
        "        print(n,mod)\n",
        "    wrf_number=int(input())    \n",
        "    met_model=met_models[wrf_number]\n",
        "    \n",
        "    #select station\n",
        "    stations_directory=\"/content/drive/MyDrive/Colab Notebooks/ML_database/met_stations/\"\n",
        "    station_list = [filename for filename in os.listdir(stations_directory) \n",
        "                    if filename.endswith('.csv') or filename.endswith('.pat')]\n",
        "    print(\"Select station number\\n\")\n",
        "    for n,mod in zip(np.arange(0,len(station_list)),station_list):\n",
        "        print(n,mod)\n",
        "    station_number=int(input())    \n",
        "    met_station=station_list[station_number] \n",
        "    \n",
        "    \n",
        "    #get model variable independent or X and D lat lon too!!!\n",
        "    # file csv or parquet\n",
        "    if met_model.endswith( \".csv\"):\n",
        "        df_x=pd.read_csv(wrf_directory+met_model,index_col=\"time\")\n",
        "    else:\n",
        "        df_x=pd.read_parquet(wrf_directory+met_model).set_index(\"time\")\n",
        "        \n",
        "    #get observed variables or Y\n",
        "    # file csv or parquet\n",
        "    if met_station.endswith( \".csv\"):\n",
        "        df_y=pd.read_csv(stations_directory+met_station,index_col=\"time\")\n",
        "    else:\n",
        "        df_y=pd.read_parquet(stations_directory+met_station).set_index(\"time\")\n",
        "        \n",
        "    #only data at the same time\n",
        "    df_all=pd.concat([df_x,df_y],axis=1).dropna()\n",
        "    \n",
        "    \n",
        "    #select time\n",
        "    month_s = input(\"select month (y/n): \")\n",
        "    if month_s==\"y\":\n",
        "      df_all.index = pd.to_datetime(df_all.index)\n",
        "      months= input(\"months in brakets from 1 to 12 as [1,2,3] \")\n",
        "      df_all=df_all.query(\"index.dt.month in \" + str(months))\n",
        "    #df_all.query(\"index.dt.year in [2015] and mod0>2\")\n",
        "    #df_all=df_all.query(\"index.dt.hour >=0 and index.dt.month in [12,1,2]\")\n",
        "    #df_all=df_all.drop(['Unnamed: 0'],axis=1)\n",
        "    print(\"independent variables :\",df_x.columns[0:23].to_list());\n",
        "    print(\"dependent variables:\",df_y.columns.to_list());\n",
        "    print(df_all.info(verbose=True));\n",
        "    \n",
        "    #get coordinates the n nearest points\n",
        "    df_r=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/ML_database/wrf_database/\"+\"distan_\"+met_model[:-6]+\".csv\")\n",
        "\n",
        "    #create a dataframe with the coordinates (lat,lon, distance) dataframe first row station coordinates\n",
        "\n",
        "    df_coor=pd.concat([pd.DataFrame({\"lat\":[df_r[\"lat_st\"][0]],\"lon\":[df_r[\"lon_st\"][0]],\n",
        "                                 \"distance\":[0]}),df_r]).reset_index(drop = True)[[\"lat\",\"lon\",\"distance\"]]\n",
        "    \n",
        "    return met_model , met_station ,df_all ,df_coor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSLJmI0wflwF"
      },
      "source": [
        "**Labels for Y variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezYfZ2j2e0_K"
      },
      "source": [
        "def temp_o (Y_raw,y_var):   \n",
        "        #temperatures dry and dew point\n",
        "        interval=pd.IntervalIndex.from_tuples([(-243,271),(271, 273),(273,275),(275,277),(277,279),(279,281),\n",
        "                                               (281,283),(283,285),(285,287),(287,289),(289,291),(291,293),\n",
        "                                               (293,295),(295,297),(297,299),(299,301),(301,303),(303,305),(305,307)\n",
        "                                               ,(307,309),(309,311),(311,313),(313,315),(315,317),(317,319),(319,321)\n",
        "                                               ,(321,333)])\n",
        "        \n",
        "        labels=[\"(-30,-2]\",\"(-2,0]\",\"(0,2]\",\"(2,4]\",\"(4,6]\",\"(6,8]\",\"(8,10]\",\"(10,12]\",\n",
        "                \"(12,14]\",\"(14,16]\",\"(16,18]\",\"(18,20]\",\"(20,22]\",\"(22,24]\",\"(24,26]\",\"(26,28]\",\n",
        "                \"(28,30]\",\"(30,32]\",\"(32,34]\",\"(34,36]\",\"(36,38]\",\"(38,40]\",\"(40,42]\",\n",
        "                \"(42,44]\",\"(44,46]\",\"(46,60]\"]\n",
        "        df_l=pd.DataFrame()\n",
        "        df_l[y_var+\"_l\"]=pd.cut(round(Y_raw,0), bins=interval,retbins=False,labels=labels,precision=3)\n",
        "        df_l[y_var+\"_l\"]=df_l[y_var+\"_l\"].map({a:b for a,b in zip(interval,labels)})\n",
        "        Y=df_l[y_var+\"_l\"]\n",
        "        return interval,labels,Y\n",
        "    \n",
        "def skyl1_o (Y_raw,y_var):\n",
        "    \n",
        "    #clouds height\n",
        "    Y_raw=pd.to_numeric(Y_raw, errors=\"coerce\")*3.28084\n",
        "    #median 1500 feet\n",
        "    interval=pd.IntervalIndex.from_tuples([(-1, 1500),(1500,7000)])\n",
        "    labels=[\"<=1500ft\",\">1500ft\"]\n",
        "    df_l=pd.DataFrame()\n",
        "    df_l[y_var+\"_l\"]=pd.cut(Y_raw, bins=interval,retbins=False,labels=labels)\n",
        "    df_l[y_var+\"_l\"]=df_l[y_var+\"_l\"].map({a:b for a,b in zip(interval,labels)})\n",
        "    df_l[y_var+\"_l\"]=df_l[y_var+\"_l\"].astype(str).replace(\"nan\",\"NC1\")\n",
        "    Y=df_l[y_var+\"_l\"]\n",
        "    return interval,labels,Y\n",
        "\n",
        "\n",
        "def metar_ra_dz (Y_raw,y_var):\n",
        "\n",
        "\n",
        "    df_all[\"Y_label\"]=\"No RA/DZ\"\n",
        "    mask=df_all['wxcodes_o'].str.contains(\"RA\")\n",
        "    df_all.loc[mask,[\"Y_label\"]]=\"RA/DZ\"\n",
        "    mask=df_all['wxcodes_o'].str.contains(\"DZ\")\n",
        "    df_all.loc[mask,[\"Y_label\"]]= \"RA/DZ\"\n",
        "    \n",
        "    print(df_all.Y_label.value_counts(normalize=True))\n",
        "    \n",
        "    labels=[\"No RA/DZ\",\"RA/DZ\"]\n",
        "    interval=pd.IntervalIndex.from_tuples([(-1, 0.1),(0.1,500)])\n",
        "    Y=df_all[\"Y_label\"]\n",
        "    return interval,labels,Y\n",
        "\n",
        "\n",
        "\n",
        "def metar_fg_br (Y_raw,y_var):\n",
        "\n",
        "    df_all[\"Y_label\"]=\"No FG/BR\"\n",
        "    mask=df_all['wxcodes_o'].str.contains(\"FG\")\n",
        "    df_all.loc[mask,[\"Y_label\"]]=\"FG/BR\"\n",
        "    mask=df_all['wxcodes_o'].str.contains(\"BR\")\n",
        "    df_all.loc[mask,[\"Y_label\"]]= \"FG/BR\"\n",
        "    \n",
        "    print(df_all.Y_label.value_counts(normalize=True))\n",
        "    \n",
        "    labels=[\"No FG/BR\",\"FG/BR\"]\n",
        "    interval=pd.IntervalIndex.from_tuples([(-1, 0.1),(0.1,500)])\n",
        "    Y=df_all[\"Y_label\"]\n",
        "    return interval,labels,Y\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "def skyc1_o (Y_raw, y_var):\n",
        "    \n",
        "    #cloud cover.Warning!! VV codes empty spaces \n",
        "    #print(df_all.skyc1_o.value_counts())\n",
        "    \n",
        "    #df_all.loc[df_all.skyc1_o==\"   \",[\"skyc1_o\",\"metar_o\"]].sample(30)\n",
        "    #df_all.loc[df_all.skyc1_o==\"M\",[\"skyc1_o\",\"metar_o\"]].sample(30)\n",
        "    \n",
        "    df_l=pd.DataFrame()\n",
        "    df_all.loc[df_all.skyc1_o==\"   \",[\"skyc1_o\"]]=\"CAVOK\"\n",
        "    df_all.loc[df_all.skyc1_o==\"M\",[\"skyc1_o\"]]=\"CAVOK\" \n",
        "    df_all.loc[df_all.skyc1_o==\"VV \",[\"skyc1_o\"]]=\"VV\" \n",
        "    labels=['CAVOK', 'FEW', 'SCT', 'BKN', 'VV', 'NSC', 'OVC']\n",
        "    df_l[y_var+\"_l\"]=df_all.skyc1_o\n",
        "    interval=[]\n",
        "    Y=df_l[y_var+\"_l\"]\n",
        "    return interval, labels, Y\n",
        "    \n",
        "\n",
        "def dir_o (Y_raw, y_var):\n",
        "    \n",
        "    #wind direction in metar VRB =-1\n",
        "    df_l=pd.DataFrame()\n",
        "    interval=pd.IntervalIndex.from_tuples([(-1.5, -0.5),(-0.5,20), (20, 40), (40, 60),\n",
        "                                           (60,80),(80,100),(100,120),(120,140),(140,160),\n",
        "                                           (160,180),(180,200),(200,220),(220,240),\n",
        "                                           (240,260),(260,280),(280,300),(300,320),\n",
        "                                           (320,340),(340,360)])\n",
        "    labels=['VRB', '[0.0, 20.0]', '(20.0, 40.0]', '(40.0, 60.0]',\n",
        "           '(60.0, 80.0]', '(80.0, 100.0]', '(100.0, 120.0]', '(120.0, 140.0]',\n",
        "           '(140.0, 160.0]', '(160.0, 180.0]', '(180.0, 200.0]', '(200.0, 220.0]',\n",
        "           '(220.0, 240.0]', '(240.0, 260.0]', '(260.0, 280.0]', '(280.0, 300.0]',\n",
        "           '(300.0, 320.0]', '(320.0, 340.0]', '(340.0, 360.0]']\n",
        "    df_l[y_var+\"_l\"]=pd.cut(Y_raw, bins=interval,retbins=False,labels=labels)\n",
        "    df_l[y_var+\"_l\"]=df_l[y_var+\"_l\"].map({a:b for a,b in zip(interval,labels)})\n",
        "    Y=df_l[y_var+\"_l\"]\n",
        "    return interval, labels, Y\n",
        "    \n",
        "def spd_beaufort (Y_raw, y_var) : \n",
        "    \n",
        "    \n",
        "    #Beaufort wind intensity in m/s scale \n",
        "    df_l=pd.DataFrame()\n",
        "    interval=pd.IntervalIndex.from_tuples([(-1, 0.5), (.5, 1.5), (1.5, 3.3),(3.3,5.5),\n",
        "                                         (5.5,8),(8,10.7),(10.7,13.8),(13.8,17.1),\n",
        "                                         (17.1,20.7),(20.7,24.4),(24.4,28.4),(28.4,32.6),(32.6,60)])\n",
        "    #labels=[\"F0\",\"F1\",\"F2\",\"F3\",\"F4\",\"F5\",\"F6\",\"F7\",\"F8\",\"F9\",\"F10\",\"F11\",\"F12\"]\n",
        "    labels = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "    df_l[y_var+\"_l\"]=pd.cut(Y_raw, bins=interval, retbins = False,precision=2,labels=labels)\n",
        "    df_l[y_var+\"_l\"]=df_l[y_var+\"_l\"].map({a:b for a,b in zip(interval,labels)})\n",
        "    Y=df_l[y_var+\"_l\"]\n",
        "    return interval, labels, Y\n",
        "\n",
        "\n",
        "\n",
        "def wind_gust (Y_raw, y_var): \n",
        "    #wind_gust\n",
        "    df_l=pd.DataFrame()\n",
        "    interval=pd.IntervalIndex.from_tuples([(0, 12.8611),(12.8611,500)])\n",
        "    labels=[\"<=25KT\",\">25KT\"]\n",
        "    df_l[y_var+\"_l\"]=pd.cut(pd.to_numeric(Y_raw,errors=\"coerce\"), bins=interval,retbins=False,labels=labels)\n",
        "    df_l[y_var+\"_l\"]=df_l[y_var+\"_l\"].map({a:b for a,b in zip(interval,labels)})\n",
        "    #no gust \"NG\"\n",
        "    df_l[y_var+\"_l\"]=df_l[y_var+\"_l\"].astype(str).replace(\"nan\",\"NG\")\n",
        "    Y=df_l[y_var+\"_l\"]\n",
        "    return interval, labels, Y\n",
        "\n",
        "\n",
        "def visibility (Y_raw, y_var): \n",
        "    \n",
        "    df_l=pd.DataFrame()\n",
        "    #interval visibility in meters\n",
        "    interval=pd.IntervalIndex.from_tuples([(-1, 1000), (1000, 50000),])\n",
        "    labels=[\"<=1000 meters\", \"> 1000 meters\"]\n",
        "    #labels = [1,0]\n",
        "    df_l[y_var+\"_l\"]=pd.cut(Y_raw, bins=interval, retbins = False,precision=2,labels=labels)\n",
        "    df_l[y_var+\"_l\"]=df_l[y_var+\"_l\"].map({a:b for a,b in zip(interval,labels)})\n",
        "    Y=df_l[y_var+\"_l\"]\n",
        "    return interval, labels, Y\n",
        "\n",
        "\n",
        "\n",
        "def prec_accumulated_1_hour_before (Y_raw, y_var): \n",
        "    \n",
        "    df_l=pd.DataFrame()\n",
        "    #prec accumulated hour before\n",
        "    interval=pd.IntervalIndex.from_tuples([(-1,0.1),(0.1,100)])\n",
        "    labels=[0,1]\n",
        "    #labels=[\"no rain\", \"rain\"]\n",
        "    df_l[y_var+\"_l\"]=pd.cut(Y_raw, bins=interval, retbins = False,precision=2,labels=labels)\n",
        "    df_l[y_var+\"_l\"]=df_l[y_var+\"_l\"].map({a:b for a,b in zip(interval,labels)})\n",
        "    Y=df_l[y_var+\"_l\"]\n",
        "    return interval, labels, Y\n",
        "\n",
        "def spd_o_metar (Y_raw, y_var):\n",
        "    df_l=pd.DataFrame()\n",
        "    #metar wind intensity\n",
        "    interval=pd.IntervalIndex.from_tuples([(-1.5, 1.55),(1.55,2.60), (2.60, 4.20), (4.20, 7.72),\n",
        "                                           (7.72,100)])\n",
        "    labels=['<=3KT', '(3KT-5KT]', '(5KT-8KT]', '(8KT-15KT]','>15KT']\n",
        "    df_l[y_var+\"_l\"]=pd.cut(Y_raw, bins=interval,retbins=False,labels=labels)\n",
        "    df_l[y_var+\"_l\"]=df_l[y_var+\"_l\"].map({a:b for a,b in zip(interval,labels)})\n",
        "    Y=df_l[y_var+\"_l\"]\n",
        "    return interval, labels, Y\n",
        "    \n",
        "\n",
        "def fr (Y_raw, y_var):\n",
        "    df_l=pd.DataFrame()\n",
        "    df_l[y_var+\"_l\"]  = Y_raw\n",
        "    labels=[\"VFR\",\"MVFR\",\"IFR\",\"LIFR\"]\n",
        "    interval=[]\n",
        "    Y=df_l[y_var+\"_l\"]\n",
        "    return interval, labels, Y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoUV84W-hUvo"
      },
      "source": [
        "**Label Y variable**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lau-PFXGg8gb"
      },
      "source": [
        "def label_y():\n",
        "     \n",
        "        \n",
        "    \"\"\"Defining independent variable and label Y \"\"\"\n",
        "    \n",
        "    import numpy as np\n",
        "    \n",
        "    \n",
        "    y_variables=[columns for columns in df_all.columns if columns.endswith(\"_o\")]\n",
        "    for n,mod in zip(np.arange(0,len(y_variables)),y_variables):\n",
        "        print(n,mod)\n",
        "    y_number=int(input(\"Select independent variable number\\n\"))    \n",
        "    y_var=y_variables[y_number]\n",
        "    Y_raw=df_all[y_var]\n",
        "       \n",
        "    \n",
        "    if y_var==\"skyl1_o\":\n",
        "       interval,labels,Y =skyl1_o (Y_raw, y_var)\n",
        "    if y_var==\"temp_o\":\n",
        "       interval,labels,Y =temp_o (Y_raw, y_var)\n",
        "    if y_var==\"skyc1_o\":\n",
        "       interval,labels,Y =skyc1_o (Y_raw, y_var)  \n",
        "    if y_var==\"wxcodes_o\":\n",
        "       functions=[metar_fg_br,metar_ra_dz]\n",
        "       for n,mod in zip(np.arange(0,len(functions)),functions):\n",
        "           print(n,mod)\n",
        "       f_number=int(input(\"Select label function number\\n\")) \n",
        "       interval,labels,Y =functions[f_number](Y_raw,y_var)\n",
        "        \n",
        "    if y_var==\"dir_o\":\n",
        "       interval,labels,Y = dir_o (Y_raw,y_var)  \n",
        "    if y_var==\"spd_o\":\n",
        "       functions=[spd_beaufort, spd_o_metar]\n",
        "       for n,mod in zip(np.arange(0,len(functions)),functions):\n",
        "           print(n,mod)\n",
        "       f_number=int(input(\"Select label function number\\n\")) \n",
        "       interval,labels,Y =functions[f_number](Y_raw,y_var)    \n",
        "         \n",
        "    if y_var==\"wind_gust_o\":\n",
        "       interval,labels,Y = wind_gust (Y_raw,y_var) \n",
        "    if y_var==\"visibility_o\":\n",
        "       interval,labels,Y = visibility (Y_raw,y_var) \n",
        "    if y_var==\"prec_accumulated_1_hour_before_o\":\n",
        "       interval,labels,Y = prec_accumulated_1_hour_before (Y_raw,y_var)   \n",
        "    if y_var==\"fr_o\":\n",
        "       interval,labels,Y = fr (Y_raw,y_var)   \n",
        "    \n",
        "    \n",
        "    #show results\n",
        "    df_all[\"Y_label\"]=Y\n",
        "    print(\"df_all Y_label column and \"+y_var)\n",
        "    pd.set_option('max_rows', 100)\n",
        "    print(df_all[[\"Y_label\",y_var]].sample(100))\n",
        "\n",
        "    return interval , labels ,df_all , y_var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7zsct27u5Yg"
      },
      "source": [
        "**Meteorological model score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMWqjZQtvNBA"
      },
      "source": [
        "def metmodel_score (df_all,y_var):\n",
        "    \"\"\"check meteorological model \"\"\"\n",
        "    import math\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "    from scipy.stats import entropy\n",
        "    from sklearn.metrics import classification_report\n",
        "    \n",
        "    x_and_y_same=input(\"met model same variables than independent (True or False)\\n\")\n",
        "    if x_and_y_same == \"True\":\n",
        "        print (df_all.columns [0:25])\n",
        "        lstx=[]\n",
        "        x_number=int(input(\"independent variables number\\n\"))\n",
        "        for i in range(0, x_number):\n",
        "            ele = str(input(\"variable number {}: \\n\".format(i)))\n",
        "            lstx.append(ele)\n",
        "        X=df_all[lstx] \n",
        "        met_var_sc={}\n",
        "        for c in X.columns:\n",
        "            df_l=pd.DataFrame()\n",
        "            print(c)\n",
        "            sc_list=[]\n",
        "            \n",
        "            df_l[c+\"_l\"]=pd.cut(X[c],bins = interval,precision=2).astype(str)\n",
        "            df_l[c+\"_l\"]=df_l[c+\"_l\"].map({a:b for a,b in zip(interval.astype(str),labels)})\n",
        "            df_l[y_var+\"_l\"]=df_all[\"Y_label\"]\n",
        "            global_sc=pd.crosstab(df_l[y_var+\"_l\"],df_l[c+\"_l\"], margins=True,)\n",
        "            sc_list.append(global_sc)\n",
        "            column_sc=pd.crosstab(df_l[y_var+\"_l\"],df_l[c+\"_l\"], margins=True,normalize=\"columns\")\n",
        "            \n",
        "            column_sc=column_sc.append(pd.DataFrame(entropy(column_sc,base=2)/(math.log2(column_sc.shape[0])),columns=[\"entropy/entropy.max\"],\n",
        "                     index=column_sc.columns).T)\n",
        "            sc_list.append(column_sc)\n",
        "            index_sc=pd.crosstab(df_l[y_var+\"_l\"],df_l[c+\"_l\"], margins=True,normalize=\"index\")\n",
        "            sc_list.append(index_sc)\n",
        "            clas_sc=pd.DataFrame(classification_report(df_l[y_var+\"_l\"].astype(str),df_l[c+\"_l\"].astype(str),output_dict=True)).T\n",
        "            sc_list.append(clas_sc)\n",
        "            met_var_sc[c]=sc_list\n",
        "            \n",
        "            fig, axs = plt.subplots(3,figsize = (16,18))\n",
        "            sns.heatmap(global_sc,annot=True,ax=axs[0],cmap=\"YlGnBu\",fmt='.0f',)\n",
        "            sns.heatmap(column_sc[:-1],annot=True,ax=axs[1],cmap=\"YlGnBu\",fmt='.0%')\n",
        "            sns.heatmap(index_sc,annot=True,ax=axs[2],cmap=\"YlGnBu\",fmt=\".0%\")\n",
        "            \n",
        "            print(clas_sc)\n",
        "            print(\"*************************************************************\")\n",
        "    else:\n",
        "        met_var_sc={}\n",
        "                \n",
        "    return met_var_sc ,x_and_y_same"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU2CEALUhlDT"
      },
      "source": [
        "*** PCA and model selection***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6CfuJOahYY-"
      },
      "source": [
        "def selectx_pca_train(df_all):\n",
        "\n",
        "    #title sklearn version and update\n",
        "    import sklearn\n",
        "    print(sklearn.__version__)\n",
        "    #!pip install -U scikit-learn\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "    from lightgbm.sklearn import LGBMClassifier\n",
        "    from catboost import CatBoostClassifier\n",
        "    from sklearn.ensemble import ExtraTreesClassifier\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.linear_model import SGDClassifier\n",
        "    from sklearn.ensemble import BaggingClassifier\n",
        "    from sklearn import svm\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.neural_network import MLPClassifier\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from xgboost import XGBClassifier\n",
        "    from imblearn.over_sampling import SMOTE \n",
        "    \n",
        "       \n",
        "    if 'Unnamed: 0' in df_all.columns:\n",
        "      df_all=df_all.drop(columns=['Unnamed: 0'])\n",
        "    \n",
        "    print(\"model variables\\n\",[ele for ele in df_all.columns if not ele.endswith(\"_o\")][:-2])\n",
        "    all_x_var=input (\"all x variables? (y/n)\\n\")\n",
        "    \n",
        "    if all_x_var==\"y\":\n",
        "        X=df_all[[ele for ele in df_all.columns if not ele.endswith(\"_o\")][:-2]]\n",
        "        x_var=X.columns\n",
        "        \n",
        "    else:\n",
        "        lsx=input(\"list of x variables\\n\").replace(\"'\",\"\").replace(\" \",\"\").replace(\"\\n\",\"\")\n",
        "        x_var=list(lsx.split(\",\"))\n",
        "        X=df_all[x_var]\n",
        "        \n",
        "        \n",
        "    PCA_n=int(input(\"PCA number less than {}\\n\".format(len(x_var)))) \n",
        "    \n",
        "    #split better stratify=Y.values\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X.values,df_all.Y_label.values,\n",
        "                                                        test_size=0.1,\n",
        "                                                        #stratify=df_all.Y_label.values,\n",
        "                                                        random_state=1)\n",
        "    \n",
        "    #scaler X\n",
        "    scaler=StandardScaler().fit(x_train)\n",
        "    x_sc=scaler.transform(x_train)\n",
        "    \n",
        "    #pca \n",
        "    pca = PCA(n_components=PCA_n,svd_solver='arpack',random_state=1)\n",
        "    x_pca = pca.fit_transform(x_sc)\n",
        "    \n",
        "    #ml models \n",
        "    models=[KNeighborsClassifier(n_neighbors=3),  XGBClassifier(n_estimators=50),\n",
        "            BaggingClassifier(),LogisticRegression(), LGBMClassifier(n_estimators=200),\n",
        "            MLPClassifier(hidden_layer_sizes=(PCA_n+10,),verbose=True,early_stopping=False,max_iter=2500,alpha=0.001),\n",
        "            svm.SVC(kernel='rbf', class_weight={1: 7}, cache_size=1500, C=1,gamma=100),\n",
        "            SGDClassifier(eta0=100, class_weight= {1: 0.4, 0: 0.6}, alpha= 0.0001),\n",
        "            DecisionTreeClassifier(random_state=1), CatBoostClassifier(),\n",
        "            LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=0.7,solver='lsqr', store_covariance=False, tol=0.0001),       \n",
        "            ExtraTreesClassifier(n_estimators=25,class_weight=\"balanced\"), RandomForestClassifier()]\n",
        "    for i in range (0,len(models)):\n",
        "        print (i ,\": \", models[i])\n",
        "    model_number= int(input (\"Enter a model number\\n\"))\n",
        "    model=models[model_number]\n",
        "    \n",
        "    # balancing?\n",
        "    balan= input(\"balance train data ? (y/n)\\n\")\n",
        "    if balan==\"y\":\n",
        "        X_res, y_res =SMOTE().fit_resample(x_pca,y_train)\n",
        "        model.fit(X_res,y_res)\n",
        "        y_pred=model.predict(pca.transform(scaler.transform(x_test)))\n",
        "        \n",
        "    else:\n",
        "        # Train the model using  pca\n",
        "        model.fit(x_pca,y_train)\n",
        "        y_pred=model.predict(pca.transform(scaler.transform(x_test)))\n",
        "    \n",
        "    return x_var, scaler, pca, model, y_pred, y_test\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyVBtyp0mVlG"
      },
      "source": [
        "**Machine learning score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5zWCVw_mTZv"
      },
      "source": [
        "def m_learning_sc (y_test, y_pred, x_var, scaler, pca, df_all):\n",
        "    \n",
        "    \"\"\" Machine learning score\"\"\"\n",
        "    \n",
        "    \n",
        "    from scipy.stats import entropy\n",
        "    from sklearn.metrics import classification_report\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import math\n",
        "    \n",
        "    global_ml=pd.crosstab(y_test,y_pred,margins=True,)\n",
        "    column_ml=pd.crosstab(y_test,y_pred,margins=True,normalize=\"columns\")\n",
        "    \n",
        "    column_ml=column_ml.append(pd.DataFrame(entropy(column_ml,base=2)/(math.log2(column_ml.shape[0])),columns=[\"entropy/entropy.max\"],\n",
        "                index=column_ml.columns).T) \n",
        "    \n",
        "    index_ml=pd.crosstab(y_test,y_pred, margins=True,normalize=\"index\")\n",
        "    \n",
        "    clas_ml=pd.DataFrame(classification_report(y_test,y_pred,output_dict=True)).T\n",
        "    \n",
        "    fig, axs = plt.subplots(3,figsize = (12,14))\n",
        "    sns.heatmap(global_ml,annot=True,ax=axs[0],cmap=\"YlGnBu\",fmt='.0f',)\n",
        "    sns.heatmap(column_ml[:-1],annot=True,ax=axs[1],cmap=\"YlGnBu\",fmt='.0%')\n",
        "    sns.heatmap(index_ml,annot=True,ax=axs[2],cmap=\"YlGnBu\",fmt=\".0%\")\n",
        "    print(clas_ml)\n",
        "    return global_ml, column_ml, index_ml , clas_ml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4iHdpEYuyFB"
      },
      "source": [
        "**Save algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "456eCZK1u9bA"
      },
      "source": [
        "def save_al():\n",
        "    \n",
        "    #save scaler, pca and algorithm\n",
        "    import pickle\n",
        "    from sklearn.model_selection import ShuffleSplit\n",
        "    from sklearn.model_selection import cross_validate\n",
        "    \n",
        "    print(\"cross validation. waiting...\")\n",
        "    cv = ShuffleSplit(n_splits=5, test_size=0.1, random_state=100)\n",
        "    cros_val=cross_validate(model, pca.transform(scaler.transform(df_all[x_var])), df_all.Y_label, cv=cv,scoring=[\"accuracy\",'f1_macro',\"f1_weighted\"]) \n",
        "    print(\"f1_weighted: %0.2f (+/- %0.2f)\" % (cros_val['test_f1_weighted'].mean(), cros_val['test_f1_weighted'].std() * 2))\n",
        "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (cros_val['test_accuracy'].mean(), cros_val['test_accuracy'].std() * 2))\n",
        "        \n",
        "       \n",
        "    \"\"\"**Save algorithm**\"\"\"\n",
        "    \n",
        "    save_a=input(\"save algorithm (y/n)\\n\")\n",
        "    if save_a==\"y\":\n",
        "        \n",
        "        abstract=str(input(\"abtract (sklearn version...)?\\n\"))\n",
        "        if x_and_y_same==\"True\":\n",
        "            \n",
        "            met_ml={\"scaler\":scaler,\"pca\":pca,\"model\":model,\"Confusion matrix\":global_ml,\"Precision\":column_ml,\n",
        "            \"Recall\":index_ml,\"Classification report\":clas_ml,\"met_var_sc\":met_var_sc,\"x_and_y_same\":True,\n",
        "            \"abstract\":abstract,\"D\":int(met_model[-5:-4]),\"interval\":interval,\"x_var\":x_var,\n",
        "            \"y_var\":y_var,\"labels\":labels,\"cros_val\":cros_val,\"coor\":df_coor,\"mod_res\":\"d03\"}\n",
        "        else:\n",
        "            \n",
        "            met_ml={\"scaler\":scaler,\"pca\":pca,\"model\":model,\"Confusion matrix\":global_ml,\"Precision\":column_ml,\n",
        "            \"Recall\":index_ml,\"Classification report\":clas_ml,\"x_and_y_same\":False,\"abstract\":abstract,\n",
        "            \"D\":int(met_model[-5:-4]),\"y_var\":y_var,\"x_var\":x_var,\n",
        "            \"labels\":labels,\"cros_val\":cros_val,\"coor\":df_coor,\"mod_res\":\"d03\"}\n",
        "        \n",
        "        file_name=input(\"algorithm filename (variable-station-(d0, d1...)?\\n\")\n",
        "        pickle.dump(met_ml, open(\"/content/drive/MyDrive/Colab Notebooks/algorithms/\"+file_name+\".al\", 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}